{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ba8afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T08:33:44.993877Z",
     "iopub.status.busy": "2025-11-26T08:33:44.993467Z",
     "iopub.status.idle": "2025-11-26T08:33:51.533407Z",
     "shell.execute_reply": "2025-11-26T08:33:51.532072Z"
    },
    "papermill": {
     "duration": 6.546174,
     "end_time": "2025-11-26T08:33:51.535674",
     "exception": false,
     "start_time": "2025-11-26T08:33:44.989500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install ddgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0147508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T08:33:51.541729Z",
     "iopub.status.busy": "2025-11-26T08:33:51.541249Z",
     "iopub.status.idle": "2025-11-26T08:33:56.822119Z",
     "shell.execute_reply": "2025-11-26T08:33:56.820883Z"
    },
    "papermill": {
     "duration": 5.286178,
     "end_time": "2025-11-26T08:33:56.824237",
     "exception": false,
     "start_time": "2025-11-26T08:33:51.538059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables set.\n",
      "‚úÖ Kaggle API Authenticated.\n",
      "‚úÖ Gemini API Authenticated.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: SETUP & AUTHENTICATION (CORRECT ORDER) ---\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# 1. Retrieve Secrets & Set Env Vars FIRST\n",
    "# (We do this BEFORE importing the kaggle library to prevent the crash)\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    # Get secrets\n",
    "    gemini_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    k_user = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "    k_key = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "\n",
    "    # Set Environment Variables\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = k_user\n",
    "    os.environ[\"KAGGLE_KEY\"] = k_key\n",
    "    print(\"‚úÖ Environment variables set.\")\n",
    "    \n",
    "    # 2. NOW it is safe to import the Kaggle API\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    k_api = KaggleApi()\n",
    "    k_api.authenticate()\n",
    "    print(\"‚úÖ Kaggle API Authenticated.\")\n",
    "\n",
    "    # 3. Authenticate Gemini\n",
    "    genai.configure(api_key=gemini_key)\n",
    "    print(\"‚úÖ Gemini API Authenticated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab831a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T08:33:56.830387Z",
     "iopub.status.busy": "2025-11-26T08:33:56.829897Z",
     "iopub.status.idle": "2025-11-26T08:36:46.283987Z",
     "shell.execute_reply": "2025-11-26T08:36:46.282872Z"
    },
    "papermill": {
     "duration": 169.460175,
     "end_time": "2025-11-26T08:36:46.286614",
     "exception": false,
     "start_time": "2025-11-26T08:33:56.826439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING KAGGLE COMMAND FOR: rsna-intracranial-aneurysm-detection\n",
      "==================================================\n",
      "üïµÔ∏è [Scout] Analyzing competition: rsna-intracranial-aneurysm-detection...\n",
      "üì° [Forum] Scanning discussions for: rsna-intracranial-aneurysm-detection...\n",
      "üë®‚Äçüíª [Notebook] Hunting for top code...\n",
      "   (Found: RSNA Aneurysm Detection Demo Submission)\n",
      "üß† [Strategist] Formulating plan...\n",
      "üèóÔ∏è [Builder] Writing final solution.py...\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## üìÑ STRATEGY REPORT\n",
       "Alright team, let's dissect this RSNA Intracranial Aneurysm Detection challenge. This is a classic 3D medical image problem, and getting it right means paying meticulous attention to data handling, robust model design, and a bulletproof validation strategy. The 85GB data size is substantial, indicating a need for efficient processing.\n",
       "\n",
       "Here's my winning strategy report:\n",
       "\n",
       "---\n",
       "\n",
       "### Winning Strategy Report: RSNA Intracranial Aneurysm Detection\n",
       "\n",
       "As Kaggle Grandmasters, our goal is to deliver a robust, high-performing solution that generalizes well to unseen patient data. The Weighted Log Loss metric demands not just accuracy but well-calibrated probability predictions across multiple aneurysm locations.\n",
       "\n",
       "#### 1. Critique of the Baseline\n",
       "\n",
       "The provided `kaggle_evaluation.rsna_inference_server` baseline is, frankly, not a baseline in the machine learning sense. It's a **placeholder demonstrating the submission system's API**.\n",
       "\n",
       "*   **No Predictive Logic:** It contains no actual code for detecting aneurysms or processing imaging data beyond iterating through file paths.\n",
       "*   **No Image Processing:** It doesn't load pixel data, perform Hounsfield Unit (HU) normalization, re-sample to a common spacing, or stack DICOM slices into a 3D volume. These are fundamental steps for any 3D CT analysis.\n",
       "*   **No Model:** There's no neural network, no classical ML algorithm, nor any inference logic whatsoever.\n",
       "*   **Purpose:** Its sole purpose is to illustrate the `predict(series_path)` function signature and the expected `pl.DataFrame` or `pd.DataFrame` output format with the `LABEL_COLS`.\n",
       "\n",
       "**Conclusion:** We must build the entire machine learning pipeline from scratch, starting with robust data loading and pre-processing, followed by a sophisticated 3D deep learning model. The baseline only serves as a guide for the *inference environment interaction*.\n",
       "\n",
       "#### 2. Specific Model Architecture\n",
       "\n",
       "Given the task of detecting and localizing aneurysms across multiple specific anatomical locations in 3D CT scans, we need a powerful 3D Convolutional Neural Network (CNN) architecture. The multi-label nature (13 specific locations + \"Aneurysm Present\") necessitates a model capable of learning distinct features for each region.\n",
       "\n",
       "**Recommended Architecture: 3D ResNeXt-50 with Multi-Heads**\n",
       "\n",
       "*   **Base Architecture:** A **3D ResNeXt-50** will serve as our primary feature extractor. ResNeXt models are known for their strong performance, efficiency, and ability to learn rich, diverse features through \"cardinality\" (group convolutions). Adapting it to 3D allows it to naturally process volumetric data.\n",
       "*   **Why 3D ResNeXt-50?**\n",
       "    *   **Volumetric Understanding:** 3D convolutions are essential for capturing spatial relationships and contextual information within the CT volume, crucial for aneurysm detection which relies on shape, size, and surrounding vessel structures.\n",
       "    *   **Performance:** ResNeXt variants have consistently shown top performance in medical imaging tasks, balancing depth, width, and computational cost effectively.\n",
       "    *   **Feature Richness:** The grouped convolutions improve feature representation without a proportional increase in parameters compared to standard convolutions.\n",
       "*   **Input & Preprocessing:**\n",
       "    *   All DICOM slices for a `SeriesInstanceUID` will be stacked into a single 3D volume.\n",
       "    *   The volume will be resampled to a consistent isotropic spacing (e.g., `1.0 x 1.0 x 1.0 mm¬≥`) to standardize input resolution.\n",
       "    *   Hounsfield Units (HU) will be normalized and clipped (e.g., `[-100, 250]` for brain tissue, then min-max scaled to `[0, 1]`) to focus on relevant tissue and make intensities consistent.\n",
       "    *   Input volumes will be center-cropped or padded to a fixed size (e.g., `128x128x128` or `256x256x256`) to fit GPU memory.\n",
       "*   **Output Heads:** The model will branch into multiple independent classification heads after the main ResNeXt backbone.\n",
       "    *   One sigmoid-activated output neuron for each of the 13 specific aneurysm location labels.\n",
       "    *   One additional sigmoid-activated output neuron for the global `Aneurysm Present` label.\n",
       "    *   Each head will predict a probability, allowing for multi-label classification.\n",
       "*   **Loss Function:** A combination of Binary Cross-Entropy (BCE) loss for each label, potentially weighted to account for class imbalance (especially for the rare aneurysm types).\n",
       "\n",
       "**Framework:** We will leverage `MONAI` (Medical Open Network for AI) for its optimized 3D building blocks, pre-trained weights (if available, or for related tasks), and robust data loading/augmentation pipelines, accelerating development and ensuring best practices.\n",
       "\n",
       "#### 3. Specific Data Augmentation / Feature Engineering Technique\n",
       "\n",
       "**Technique: 3D Elastic Deformations**\n",
       "\n",
       "*   **Description:** Elastic deformations are a powerful geometric augmentation technique where a dense displacement field is applied to the image (and corresponding labels/masks, if applicable). This simulates realistic non-rigid transformations that occur in biological tissues, such as variations in patient posture, anatomical variability, or subtle organ motion.\n",
       "*   **Implementation:** We will generate a random displacement field (e.g., using Gaussian smoothing of random vectors) and then use interpolation to apply these deformations to the 3D CT volume.\n",
       "*   **Why it's crucial here:**\n",
       "    1.  **Mimics Anatomical Variability:** Brain structures and blood vessel paths naturally vary from person to person. Elastic deformations introduce these subtle, non-linear shape changes, making the model more robust to inter-patient anatomical differences.\n",
       "    2.  **Improves Generalization:** By exposing the model to slightly warped versions of existing aneurysms and healthy structures, it learns to identify aneurysms based on their intrinsic features rather than specific spatial configurations tied to the training data.\n",
       "    3.  **Data Augmentation for Limited Data:** Medical imaging datasets, while large in GB, can be limited in terms of unique aneurysm cases. Elastic deformations effectively increase the diversity of our training data without requiring more raw scans.\n",
       "\n",
       "We will combine this with other standard augmentations like random rotations, translations, scaling, and intensity adjustments (e.g., random brightness/contrast, Gaussian noise).\n",
       "\n",
       "#### 4. Validation Strategy\n",
       "\n",
       "**Strategy: Patient-Stratified K-Fold Cross-Validation**\n",
       "\n",
       "*   **Core Principle:** It is absolutely critical to split data at the *patient level*. Any series from the same patient must belong exclusively to either the training or validation set. Failing to do so would lead to data leakage, where the model might \"memorize\" patient-specific characteristics rather than generalize to new patients, resulting in overly optimistic validation scores that don't reflect real-world performance.\n",
       "*   **Steps:**\n",
       "    1.  **Identify Unique Patients:** Extract all unique `PatientID`s from the training metadata.\n",
       "    2.  **Stratification Target:** The `Aneurysm Present` label is the most critical for stratification, as aneurysm cases are likely rare. For each `PatientID`, determine if *any* of their associated series contain an aneurysm (`Aneurysm Present = 1`). We can create a stratification target that balances the number of \"aneurysm-positive\" and \"aneurysm-negative\" patients across folds.\n",
       "    3.  **K-Fold Split:** Divide the unique `PatientID`s into `K` (e.g., K=5) folds, ensuring that the distribution of aneurysm-positive patients is as even as possible across all folds.\n",
       "    4.  **Training and Validation:** For each fold `i` (from 1 to K):\n",
       "        *   **Training Set:** All series belonging to patients in the `K-1` other folds.\n",
       "        *   **Validation Set:** All series belonging to patients in fold `i`.\n",
       "    5.  **Out-Of-Fold (OOF) Predictions:** During training, generate predictions for the validation set of each fold. These OOF predictions can be concatenated to form a complete set of predictions for the entire training dataset, which is invaluable for:\n",
       "        *   Estimating true model performance.\n",
       "        *   Ensembling multiple models (e.g., blending OOF predictions from different folds/models).\n",
       "        *   Post-processing and calibration of probabilities.\n",
       "    6.  **Ensembling (Optional but Recommended):** Train multiple models (different seeds, slightly different architectures, or data splits) and average their predictions on the test set. OOF predictions help confirm which models blend well.\n",
       "\n",
       "This strategy will provide a robust estimate of our model's performance on unseen patients and is the gold standard for medical image competitions.\n",
       "\n",
       "---\n",
       "\n",
       "This comprehensive strategy, from data handling and model architecture to augmentation and rigorous validation, sets us on the path to a top solution. Iterative refinement based on validation results will be key. Let's get to work!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## üêç GENERATED CODE\n",
       "Error: Generation failed with 500 Received RST_STREAM with error code 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Code saved to 'submission.py'\n"
     ]
    }
   ],
   "source": [
    "# --- 1. IMPORTS & SETUP ---\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from ddgs import DDGS\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# --- 2. AUTHENTICATION & CONFIG ---\n",
    "def setup_system():\n",
    "    user_secrets = UserSecretsClient()\n",
    "    try:\n",
    "        # Gemini Auth\n",
    "        api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "        genai.configure(api_key=api_key)\n",
    "        \n",
    "        # Kaggle Auth\n",
    "        k_user = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "        k_key = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "        \n",
    "        # Set Env Vars (Safest method)\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = k_user\n",
    "        os.environ[\"KAGGLE_KEY\"] = k_key\n",
    "        \n",
    "        # Initialize API\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        return api\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Auth Error: {e}\")\n",
    "        return None\n",
    "\n",
    "k_api = setup_system()\n",
    "\n",
    "# UPDATED: Use the stable 1.5 Flash model\n",
    "model_name = 'gemini-2.5-flash' \n",
    "\n",
    "# Safety Config (Block None to allow medical/technical discussions)\n",
    "safety_config = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "# --- HELPER: SAFE GENERATION ---\n",
    "# --- HELPER: ROBUST STREAMING GENERATION ---\n",
    "def safe_generate(model, prompt):\n",
    "    \"\"\"\n",
    "    Uses streaming to prevent 504 Timeouts on long code generation tasks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Enable streaming\n",
    "        response_iterator = model.generate_content(prompt, safety_settings=safety_config, stream=True)\n",
    "        \n",
    "        full_text = \"\"\n",
    "        # Iterate through the chunks as they arrive\n",
    "        for chunk in response_iterator:\n",
    "            if chunk.text:\n",
    "                full_text += chunk.text\n",
    "        \n",
    "        if not full_text:\n",
    "            return \"Error: Model returned empty response.\"\n",
    "            \n",
    "        return full_text\n",
    "\n",
    "    except ValueError:\n",
    "        return \"Error: Model finished but generated no text (Silent Block).\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Generation failed with {str(e)}\"\n",
    "\n",
    "# --- 3. AGENT DEFINITIONS ---\n",
    "\n",
    "class ProblemScoutAgent:\n",
    "    def run(self, slug):\n",
    "        print(f\"üïµÔ∏è [Scout] Analyzing competition: {slug}...\")\n",
    "        try:\n",
    "            # RSNA Override\n",
    "            if \"rsna\" in slug and \"aneurysm\" in slug:\n",
    "                return \"Goal: Detect intracranial aneurysms on 3D CT. Metric: Weighted Log Loss. Data: 85GB 3D DICOM.\"\n",
    "            \n",
    "            # API Fetch\n",
    "            comps = k_api.competitions_list(search=slug.split('-')[0])\n",
    "            target = next((c for c in comps if c.ref == slug), None)\n",
    "            if target:\n",
    "                return f\"Goal: {target.description[:500]}... Metric: {target.evaluationMetric}.\"\n",
    "            return \"Could not fetch official details via API.\"\n",
    "        except Exception as e:\n",
    "            return f\"Scout Error: {e}\"\n",
    "\n",
    "class ForumScoutAgent:\n",
    "    def run(self, slug):\n",
    "        print(f\"üì° [Forum] Scanning discussions for: {slug}...\")\n",
    "        readable_name = slug.replace(\"-\", \" \")\n",
    "        query = f\"{readable_name} kaggle discussion solution tricks\"\n",
    "        try:\n",
    "            with DDGS() as ddgs:\n",
    "                results = list(ddgs.text(query, max_results=5))\n",
    "            \n",
    "            if not results: return \"No discussions found.\"\n",
    "            summary = \"\\n\".join([f\"- {r['title']}: {r['body'][:200]}...\" for r in results])\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            return f\"Forum Search Error: {e}\"\n",
    "\n",
    "class NotebookScoutAgent:\n",
    "    def run(self, slug):\n",
    "        print(f\"üë®‚Äçüíª [Notebook] Hunting for top code...\")\n",
    "        try:\n",
    "            kernels = []\n",
    "            try: kernels = k_api.kernels_list(competition=slug, sort_by='voteCount', page_size=1)\n",
    "            except: pass\n",
    "            \n",
    "            if not kernels:\n",
    "                print(\"   (Strict filter failed, trying broad search...)\")\n",
    "                kernels = k_api.kernels_list(search=slug.split('-')[0], sort_by='voteCount', page_size=1)\n",
    "            \n",
    "            if not kernels: return \"No public code found.\"\n",
    "            \n",
    "            top_k = kernels[0]\n",
    "            print(f\"   (Found: {top_k.title})\")\n",
    "            \n",
    "            k_api.kernels_pull(top_k.ref, path=\"./downloaded_code\")\n",
    "            \n",
    "            code_content = \"\"\n",
    "            files = glob.glob(\"./downloaded_code/*\")\n",
    "            for f_path in files:\n",
    "                if f_path.endswith(\".py\"):\n",
    "                    with open(f_path,'r') as f: code_content += f.read()\n",
    "                elif f_path.endswith(\".ipynb\"):\n",
    "                    with open(f_path,'r') as f:\n",
    "                        nb = json.load(f)\n",
    "                        for c in nb['cells']: \n",
    "                            if c['cell_type']=='code': code_content += \"\".join(c['source']) + \"\\n\"\n",
    "            \n",
    "            if len(code_content) < 50: return \"Code was empty.\"\n",
    "            return f\"--- CODE FROM {top_k.title} ---\\n{code_content[:25000]}\"\n",
    "        except Exception as e:\n",
    "            return f\"Code Download Error: {e}\"\n",
    "\n",
    "class StrategistAgent:\n",
    "    def run(self, slug, goal, forum_intel, code_intel):\n",
    "        print(f\"üß† [Strategist] Formulating plan...\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Act as a Kaggle Grandmaster.\n",
    "        Competition: {slug}\n",
    "        \n",
    "        1. OFFICIAL GOAL: {goal}\n",
    "        2. COMMUNITY INTEL: {forum_intel}\n",
    "        3. EXISTING CODE BASELINE: {code_intel[:2000]}... (truncated)\n",
    "        \n",
    "        Task: Write a 'Winning Strategy' report.\n",
    "        - Critique the baseline.\n",
    "        - Identify the specific model architecture we should build.\n",
    "        - Suggest 1 specific data augmentation or feature engineering technique.\n",
    "        - Define the validation strategy.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        return safe_generate(model, prompt)\n",
    "\n",
    "class CodeGeneratorAgent:\n",
    "    def run(self, strategy_report):\n",
    "        print(f\"üèóÔ∏è [Builder] Writing final solution.py...\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are an expert Python Developer.\n",
    "        \n",
    "        Based on this strategy:\n",
    "        {strategy_report}\n",
    "        \n",
    "        Write a COMPLETE, RUNNABLE 'main.py' script.\n",
    "        - Include Dataset class, Model class, and Training Loop.\n",
    "        - Use PyTorch.\n",
    "        - Handle the specific data types mentioned (e.g. Images, CSVs).\n",
    "        \"\"\"\n",
    "        \n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        return safe_generate(model, prompt)\n",
    "\n",
    "# --- 4. THE ORCHESTRATOR ---\n",
    "\n",
    "class KaggleCommandSystem:\n",
    "    def __init__(self):\n",
    "        self.scout = ProblemScoutAgent()\n",
    "        self.forum = ForumScoutAgent()\n",
    "        self.notebook = NotebookScoutAgent()\n",
    "        self.strategist = StrategistAgent()\n",
    "        self.builder = CodeGeneratorAgent()\n",
    "        \n",
    "    def execute(self, competition_slug):\n",
    "        print(f\"üöÄ STARTING KAGGLE COMMAND FOR: {competition_slug}\\n\" + \"=\"*50)\n",
    "        \n",
    "        goal_data = self.scout.run(competition_slug)\n",
    "        forum_data = self.forum.run(competition_slug)\n",
    "        code_data = self.notebook.run(competition_slug)\n",
    "        \n",
    "        strategy = self.strategist.run(competition_slug, goal_data, forum_data, code_data)\n",
    "        \n",
    "        # If strategy failed, don't try to build code\n",
    "        if \"Error\" in strategy and len(strategy) < 100:\n",
    "            return {\"strategy_report\": strategy, \"final_code\": \"Skipped due to strategy error.\"}\n",
    "            \n",
    "        final_code = self.builder.run(strategy)\n",
    "        \n",
    "        return {\n",
    "            \"strategy_report\": strategy,\n",
    "            \"final_code\": final_code\n",
    "        }\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "system = KaggleCommandSystem()\n",
    "slug = \"rsna-intracranial-aneurysm-detection\" \n",
    "# You can change 'slug' to 'hull-tactical-speed-dating' or any other active comp to test\n",
    "\n",
    "try:\n",
    "    result = system.execute(slug)\n",
    "    \n",
    "    from IPython.display import Markdown, display\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    display(Markdown(f\"## üìÑ STRATEGY REPORT\\n{result['strategy_report']}\"))\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    display(Markdown(f\"## üêç GENERATED CODE\\n{result['final_code']}\"))\n",
    "    \n",
    "    with open(\"submission.py\", \"w\") as f:\n",
    "        f.write(result['final_code'])\n",
    "    print(\"‚úÖ Code saved to 'submission.py'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fatal System Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 189.502256,
   "end_time": "2025-11-26T08:36:49.261800",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-26T08:33:39.759544",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
